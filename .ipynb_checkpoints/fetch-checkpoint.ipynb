{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving Coordinates and Hierarchy from GeoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Introduction\n",
    "This Jupyter Notebook is intended to implement GeoNames API to autopopulate coordinates and hierarchy of place names.<br>\n",
    "\n",
    "###  1. What is GeoNames?\n",
    "- **<a href='http://www.geonames.org/'>GeoNames</a>** is geographic database available for pulling in information like bounding boxes, centroids, hierarchy, children based on place name.<br> More specifically, all features are categorized into one out of 9 feature classes and further subcategorized into one out of 645 <a href='http://www.geonames.org/export/codes.html'>feature codes</a>.\n",
    "- Also, a Python library called **<a href='https://geocoder.readthedocs.io/providers/GeoNames.html'>Geocoder</a>** supports webservices below.  \n",
    "\n",
    "### 2. Limitations of GeoNames\n",
    "- GeoNames does not support searching for exact match and always returns the first one among the result. There's a great chance that it ends up with a mismatch. \n",
    "- Daily and hourly limit of web service request. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Preparation\n",
    "We will be using **Jupyter Notebook(anaconda 3)** to edit and run the script. Information on Anaconda installation can be found <a href='https://docs.anaconda.com/anaconda/install/'>here</a>. Please note that this script is running on Python 3.\n",
    "\n",
    "Before running the script, you may need to:\n",
    "### 1. Install Libraries\n",
    "- <a href='https://geocoder.readthedocs.io/providers/GeoNames.html'>Geocoder</a> `pip install geocoder`\n",
    "- <a href='https://chardet.readthedocs.io/en/latest/index.html'>chardet</a> would auto-detect the character encoding. It is incorporated to deal with non-English metadata. `pip install chardet`\n",
    "\n",
    "### 2. Restructure Directories\n",
    "- ***fetch.ipynb***\n",
    "- ***fetch.py***\n",
    "- **data** folder\n",
    "    - **code** foloder\n",
    "        - ***code.csv*** formatted in GBL Metadata Template\n",
    "        \n",
    "### 3. Inspect Output\n",
    "In order to avoid exceeding the hourly/daily limit, this script includes a function to split the csv file into multiple smaller ones so users will be executing part of the script several times to fetch the GeoNames information. The final product would be one csv file named ***code_done.csv***. \n",
    "\n",
    "> Original created on Jan 24 2021 <br>\n",
    "> @author: Yijing Zhou @YijingZhou33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Get Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geocoder.geonames as geonames\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import chardet\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Manual items to change\n",
    "Note that you need a GeoNames user account, you can register a free one <a href='http://www.geonames.org/login'>here</a>.<br>\n",
    "According to `Terms and Conditions`, the hourly limit for personal account is 1000 credits and 1 credit is 1 hit for webservice request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoNames account name\n",
    "username = 'yijingzhou'\n",
    "# try:\n",
    "#     from config import * \n",
    "# except ImportError:\n",
    "#     pass\n",
    "\n",
    "# code/name of the rawdata\n",
    "code = 'GeoNames-Test Records'\n",
    "\n",
    "# The number of records per file after splitting  \n",
    "# The recommended rowlimit is 250.  \n",
    "rowlimit = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Process Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Convert GeoBlackLight Metadata csv file to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('data', code, code + '.csv')\n",
    "\n",
    "df = pd.read_csv(filepath)\n",
    "## List of metadata fields from the GBL metadata template required in the final product.\n",
    "collist = ['Slug', 'Title', 'Spatial Coverage']\n",
    "## Alternative columns\n",
    "## Check if exists, if so then add it to the list.\n",
    "## Also more properties can be added here!\n",
    "altlist = ['Information', 'Download', 'Static image']\n",
    "collist = collist + [i for i in altlist if i in df]\n",
    "\n",
    "df = df[collist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Predict encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_encoding(file_path):\n",
    "    rawdata = open(file_path, 'rb').read()\n",
    "    result = chardet.detect(rawdata)\n",
    "    charenc = result['encoding']\n",
    "    return charenc\n",
    "\n",
    "encoding = predict_encoding(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Extract keyword(s) from place name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_placename(df):\n",
    "    df['Locator'] = ''\n",
    "    for _, row in df.iterrows():\n",
    "        if str(row['Spatial Coverage']) == 'nan':\n",
    "            #### e.g. 'Spatial Coverage' is empty\n",
    "            #### return []\n",
    "            row['Locator'] = []\n",
    "        elif '|' in row['Spatial Coverage']:\n",
    "            if ',' in row['Spatial Coverage']:\n",
    "            #### e.g. Hellam, Pennsylvania|York County, Pennsylvania|Pennsylvania\n",
    "            #### return ['Hellam, Pennsylvania', 'York County, Pennsylvania']\n",
    "                row['Locator'] = [x for x in row['Spatial Coverage'].split('|')[0:-1]]\n",
    "            else:\n",
    "            #### e.g. Asia|Europe|Arctic Circle|Arctic\n",
    "            #### return ['Asia', 'Europe', 'Arctic Circle', 'Arctic']\n",
    "                row['Locator'] = [x for x in row['Spatial Coverage'].split('|')]\n",
    "        else:\n",
    "            #### e.g. Oneida County, Wisconsin|Wisconsin\n",
    "            #### return ['Oneida County, Wisconsin']\n",
    "            row['Locator'] = [row['Spatial Coverage']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Split raw data into multiple smaller files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "## Prints out the number of files after spliting\n",
    "## Appnend the according number to each file\n",
    "## e.g. code_1.csv, code_2.csv, ...\n",
    "def split_csvs():\n",
    "    suffix = 1\n",
    "    suffixlist = []\n",
    "    for i in range(len(df)):\n",
    "        if i % rowlimit == 0:\n",
    "            suffixlist.append(suffix)\n",
    "            splitpath = os.path.join('data', code, f'{code}_{suffix}.csv')\n",
    "            df[i:i + rowlimit].to_csv(splitpath, index = False, encoding = encoding)\n",
    "            suffix += 1\n",
    "    return suffixlist\n",
    "\n",
    "splitlist = split_csvs()\n",
    "print(splitlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Fetch URI, coordinates, and hierarchy of place name from GeoNames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Fetch URI, coordinates, and hierarchy of place name from GeoNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geoid(geoname, featureClass):\n",
    "    if featureClass:\n",
    "        return geonames(geoname, featureClass = featureClass, key = username)\n",
    "    else: \n",
    "        return geonames(geoname, key = username)\n",
    "    \n",
    "def bbox(geonames_id):\n",
    "    return geonames(geonames_id, method = 'details', key = username).bbox\n",
    "\n",
    "def hierarchy(geonames_id):\n",
    "    return geonames(geonames_id, method = 'hierarchy', key = username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Status code 401 from http://api.geonames.org/searchJSON: ERROR - 401 Client Error: Unauthorized for url: http://api.geonames.org/searchJSON?q=Minneapolis&fuzzy=1.0&username=geobtaa&maxRows=1&featureClass=A\n",
      "Status code 401 from http://api.geonames.org/getJSON: ERROR - 401 Client Error: Unauthorized for url: http://api.geonames.org/getJSON?username=geobtaa&style=full\n",
      "Status code 401 from http://api.geonames.org/hierarchyJSON: ERROR - 401 Client Error: Unauthorized for url: http://api.geonames.org/hierarchyJSON?username=geobtaa\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "username = 'geobtaa'\n",
    "place = 'Minneapolis'\n",
    "geoid = geonames(place, featureClass = 'A', key = username).geonames_id\n",
    "d = geonames(geoid, method = 'details', key = username)\n",
    "h = geonames(geoid, method = 'hierarchy', key = username)\n",
    "\n",
    "print(d.bbox)\n",
    "print([r.address for r in h][1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "> Note that the code block below may need to be run multiple times for each smaller csv file. <br>\n",
    "> Also the parameter should be changed manually according to the list of split files before execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Drezdenko, Poland'] [7532994]\n",
      "['Kołobrzeg, Poland'] [7530843]\n",
      "['Międzyrzecz, Poland'] [7533483]\n",
      "['Recz, Poland'] [7533220]\n",
      "['Sulechów, Poland'] [7533164]\n",
      "['Victoria Land, Antarctica'] [6627528]\n",
      "['Walgreen Coast, Antarctica', 'Palmer Peninsula, Antarctica'] [6627723]\n",
      "['Leopold and Astrid Coast, Antarctica', 'Wilhelm II Coast, Antarctica'] [6636877, 6628103]\n",
      "['Ross Ice Shelf, Antarctica', 'Victoria Land, Antarctica'] [6638681, 6627528]\n",
      "['Marie Byrd Land, Antarctica'] [6637528]\n",
      "['Weddell Sea, Antarctica'] [4036624]\n",
      "['Enderby Land, Antarctica', 'Queen Maud Land, Antarctica'] [6630103, 6623778]\n",
      "['Wilkes Land, Antarctica', 'Victoria Land, Antarctica'] [6628107, 6627528]\n",
      "['Edith Ronne Land, Antarctica', 'Coats Land, Antarctica', 'Queen Maud Land, Antarctica'] [6624358, 6630102, 6623778]\n",
      "['Canada', 'Alaska', 'Greenland', 'Arctic'] [6251999, 5879092, 3425505, 10941926]\n",
      "-------- End --------\n"
     ]
    }
   ],
   "source": [
    "def bbox_hierarchy(suffix): \n",
    "    splitpath = os.path.join('data', code, f'{code}_{suffix}.csv')\n",
    "    outputpath = os.path.join('data', code, f'{code}_done_{suffix}.csv')\n",
    "    df = pd.read_csv(splitpath)\n",
    "    extract_placename(df)\n",
    "\n",
    "    df['GeoNames URI'] = ''\n",
    "    df['GeoNames Bbox'] = ''\n",
    "    df['GeoNames Hierarchy'] = ''\n",
    "    for _, row in df.iterrows():\n",
    "        geonames_ids = []\n",
    "        geonames = []\n",
    "        ## Filter out GeoNames id using place name(['Locator']) and featureClass \n",
    "        for x in row['Locator']:\n",
    "            geonames.append(x)\n",
    "            #### For more information about featureClass, find here: http://www.geonames.org/export/codes.html\n",
    "            #### A - country, state, region,...\n",
    "            #### H - stream, lake, ...\n",
    "            #### P - city, village,...\n",
    "            #### L - parks,area, ...\n",
    "            #### T - mountain,hill,rock,...\n",
    "            if re.search('river|lake', x, re.IGNORECASE) and ',' not in x:\n",
    "                geonames_id = geoid(x, 'H').geonames_id\n",
    "            elif re.search('Antarctica|Arctic', x, re.IGNORECASE):\n",
    "                if geoid(x, ['L', 'H', 'T']):\n",
    "                    geonames_id = geoid(x, ['L', 'H', 'T']).geonames_id\n",
    "                else:\n",
    "                    geonames_id = geoid(x, '').geonames_id\n",
    "            elif geoid(x, 'A'):\n",
    "                geonames_id = geoid(x, 'A').geonames_id\n",
    "            elif geoid(x, 'P'):\n",
    "                geonames_id = geoid(x, 'P').geonames_id\n",
    "            else:\n",
    "                geonames_id = geoid(x, '').geonames_id\n",
    "            \n",
    "            if geonames_id:\n",
    "                geonames_ids.append(geonames_id)\n",
    "            else: \n",
    "                pass\n",
    "        \n",
    "        ## Inspect each record    \n",
    "        print(geonames, geonames_ids)\n",
    "        \n",
    "        ## If column['Locator'] only includes one place name    \n",
    "        if len(geonames_ids) == 1:\n",
    "            ##### GeoNames URI #####\n",
    "            row['GeoNames URI'] = 'https://sws.geonames.org/' + str(geonames_ids[0])\n",
    "            \n",
    "            ##### GeoNames Bounding Box #####\n",
    "            bboxdic = bbox(geonames_ids[0])\n",
    "            #### Check if bounding box exists since some place names only contain centroids\n",
    "            if bboxdic:\n",
    "                bboxlist = [round(x, 2) for x in (bboxdic['southwest'] + bboxdic['northeast'])]\n",
    "                row['GeoNames Bbox'] = ', '.join(str(x) for x in bboxlist)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "            ##### GeoNames Hierarchy #####\n",
    "            hlist = [r.address for r in hierarchy(geonames_ids[0])][1:]\n",
    "            #### Check if this place is inside of U.S., if so append 'County' to county name\n",
    "            if len(hlist) > 3 and hlist[1] == 'United States':\n",
    "                hlist[3] = hlist[3] + ' County'\n",
    "            row['GeoNames Hierarchy'] = '|'.join(str(x) for x in hlist)\n",
    "        \n",
    "        ## If column['Locator'] only includes multiple place names\n",
    "        elif len(geonames_ids) > 1:\n",
    "            hlists = []\n",
    "            bboxlists = []\n",
    "            \n",
    "            ##### GeoNames URI #####\n",
    "            urilist = [('https://sws.geonames.org/' + str(x)) for x in geonames_ids]\n",
    "            row['GeoNames URI'] = ', '.join(str(x) for x in urilist)\n",
    "            \n",
    "            ##### GeoNames Bounding Box #####\n",
    "            for i in geonames_ids:\n",
    "                hlist = [r.address for r in hierarchy(i)][1:]\n",
    "                if len(hlist) > 3 and hlist[1] == 'United States':\n",
    "                    hlist[3] = hlist[3] + ' County'\n",
    "                hlists.append(hlist)\n",
    "                \n",
    "                bboxdic = bbox(i)\n",
    "                if bboxdic:\n",
    "                    bboxlist = bboxdic['southwest'] + bboxdic['northeast']\n",
    "                    bboxlists.append(bboxlist)\n",
    "                else:\n",
    "                    pass\n",
    "            #### Find the largest extend\n",
    "            if bboxlists:\n",
    "                minX = min(x[0] for x in bboxlists)\n",
    "                minY = min(x[1] for x in bboxlists)\n",
    "                maxX = max(x[2] for x in bboxlists)\n",
    "                maxY = max(x[3] for x in bboxlists)\n",
    "                combinedBbox = [round(x, 2) for x in [minX, minY, maxX, maxY]]\n",
    "                row['GeoNames Bbox'] = ', '.join(str(x) for x in combinedBbox)\n",
    "            else: \n",
    "                pass               \n",
    "            \n",
    "            ##### GeoNames Bounding Box #####\n",
    "            cols = list(range(0, max(len(i) for i in hlists)))\n",
    "            df_h = pd.DataFrame(columns = cols, data = hlists)\n",
    "            #### Merge the same hierarchy\n",
    "            #### e.g. ['Hellam, Pennsylvania', 'York County, Pennsylvania']\n",
    "            #### return North America|United States|Pennsylvania|York County|Township of Hellam\n",
    "            nonan = [','.join(list(filter(None, set(df_h[col])))) for col in cols]\n",
    "            row['GeoNames Hierarchy'] = '|'.join(str(x) for x in nonan)\n",
    "    \n",
    "    print('-------- End --------')\n",
    "    df_clean = df.drop(columns = ['Locator'])\n",
    "    df_clean.to_csv(outputpath, index = False, encoding = encoding)\n",
    "    \n",
    "bbox_hierarchy(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Merge smaller output files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_csvs():\n",
    "    ## Combine all output files into one called 'code_done.csv'\n",
    "    combined_csv = pd.concat([pd.read_csv(os.path.join('data', code, f'{code}_done_{suffix}.csv')) for suffix in splitlist])\n",
    "    outputpath = os.path.join('data', code, code + '_done.csv')\n",
    "    combined_csv.to_csv(outputpath, index = False, encoding = encoding)\n",
    "    ## Remove small output files\n",
    "    for suffix in splitlist:\n",
    "        os.remove(os.path.join('data', code, f'{code}_{suffix}.csv'))\n",
    "        os.remove(os.path.join('data', code, f'{code}_done_{suffix}.csv'))    \n",
    "\n",
    "combined_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
